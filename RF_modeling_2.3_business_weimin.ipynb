{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark Case Demo - Products Recommendation using Random Forest ###########\n",
    "\n",
    "# Question \n",
    "## What is the probability that a given customer will like a certain product?\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data \n",
    "### 1) The data set includes both product and customer Info\n",
    "###  2) Feedback column is customer's response to a certain product \n",
    "### 3) All mock-up stuff, only to highlight what's possible using PySpark scripts\n",
    "<br>\n",
    "\n",
    "![caption](data_head.png)\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps covered in this notebook: \n",
    "### 1) data pre-processing\n",
    "### 2) Using ml random forest classifier for binary classification \n",
    "### 3) Measuring performance using AUC score \n",
    "### 4) Different strategies to handle the problem of unbalanced dataset\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark, start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ensure all the 'context' things will be taken care of by your Big Data team\n",
    "tableData = sqlContext.table('db.your_table_containing_products_feedback_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_select = ['prod_price',\n",
    "               'prod_feat_1',\n",
    "               'prod_feat_2',\n",
    "               'cust_age',\n",
    "               'prod_feat_3',\n",
    "               'cust_region',\n",
    "               'prod_type',\n",
    "               'cust_sex',\n",
    "               'cust_title',\n",
    "               'feedback']\n",
    "df = tableData.select(cols_select).dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing - Combine 'Neutral's and 'Negative's into 'Negative's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Histograms below: \")\n",
    "\n",
    "responses = df.groupBy('feedback').count().collect() # list of Rows\n",
    "categories = [i[0] for i in responses]\n",
    "counts = [i[1] for i in responses]\n",
    "\n",
    "ind = np.array(range(len(categories)))\n",
    "width = 0.35\n",
    "plt.bar(ind, counts, width=width, color='r')\n",
    "\n",
    "plt.ylabel('counts')\n",
    "plt.title('Response distribution')\n",
    "plt.xticks(ind + width/2., categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms below: \n",
    "\n",
    "![caption](files/hist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "binarize = lambda x: 'Negative' if x == 'Neutral' else x\n",
    " \n",
    "udfValueToCategory = udf(binarize, StringType())\n",
    "df = df.withColumn(\"binary_response\", udfConvertResponse(\"feedback\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling NA values and casting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_select = ['prod_price',\n",
    "               'prod_feat_1',\n",
    "               'prod_feat_2',\n",
    "               'cust_age',\n",
    "               'prod_feat_3',\n",
    "               'cust_region',\n",
    "               'prod_type',\n",
    "               'cust_sex',\n",
    "               'cust_title',\n",
    "               'feedback',\n",
    "               'binary_response']\n",
    " \n",
    "df = df.select(df.prod_price.cast('float'), # convert numeric cols (int or float) into a 'int' or 'float'\n",
    "               df.prod_feat_1.cast('float'),\n",
    "               df.prod_feat_2.cast('float'),\n",
    "               df.cust_age.cast('int'),\n",
    "               *cols_select[4:])\n",
    " \n",
    "df = df.fillna({'cust_region': 'NA', 'cust_title': 'NA', 'prod_type': 'NA'}) # fill in 'N/A' entries for certain cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical col that has too many discrete values - 'prod_feat_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns[4:-2]:\n",
    "    print(col, df.select(col).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prod_feat_3 553\n",
    "cust_region 12\n",
    "prod_type 35\n",
    "cust_sex 2\n",
    "cust_title 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "COUNT_THRESHOLD = 150 # threshold to filter \n",
    " \n",
    "# create a temporary col \"count\" as counting for each value of \"prod_feat_3\"\n",
    "prodFeat3Count = df.groupBy(\"prod_feat_3\").count()\n",
    "df = df.join(prodFeat3Count, \"prod_feat_3\", \"inner\")\n",
    " \n",
    "def convertMinority(originalCol, colCount):\n",
    "    if colCount > COUNT_THRESHOLD:\n",
    "        return originalCol\n",
    "    else:\n",
    "        return 'MinorityCategory'\n",
    "    \n",
    "createNewColFromTwo = udf(convertMinority, StringType())\n",
    "df = df.withColumn('prod_feat_3_reduced', createNewColFromTwo(df['prod_feat_3'], df['count']))\n",
    "df = df.drop('prod_feat_3')\n",
    "df = df.drop('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_vec_in = ['prod_feat_3_reduced', 'cust_region', 'prod_type', 'cust_sex', 'cust_title']\n",
    "column_vec_out = ['prod_feat_3_reduced_catVec','cust_region_catVec', 'prod_type_catVec','cust_sex_catVec',\n",
    "'cust_title_catVec']\n",
    " \n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_tmp') for x in column_vec_in ]\n",
    " \n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y)\n",
    "            for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labeled set - 'features' and 'label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labeled sets\n",
    "cols_now = ['prod_price',\n",
    "            'prod_feat_1',\n",
    "            'prod_feat_2',\n",
    "            'cust_age',\n",
    "            'prod_feat_3_reduced_catVec',\n",
    "            'cust_region_catVec',\n",
    "            'prod_type_catVec',\n",
    "            'cust_sex_catVec',\n",
    "            'cust_title_catVec']\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "labelIndexer = StringIndexer(inputCol='binary_response', outputCol=\"label\")\n",
    "tmp += [assembler_features, labelIndexer]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation sets - remember to set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allData = pipeline.fit(df).transform(df)\n",
    "allData.cache()\n",
    "trainingData, testData = allData.randomSplit([0.8,0.2], seed=0) # need to ensure same split for each time\n",
    "print(\"Distribution of Pos and Neg in trainingData is: \", trainingData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Pos and Neg in trainingData is:  [Row(label=1.0, count=144014), Row(label=0.0, count=520771)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RF(labelCol='label', featuresCol='features', numTrees=200)\n",
    "fit = rf.fit(trainingData)\n",
    "transformed = fit.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = transformed.select(['probability', 'label'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC score is (@numTrees=200): 0.6425143766095695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/auc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_probs = transformed.select(\"probability\").collect()\n",
    "pos_probs = [i[0][0] for i in all_probs]\n",
    "neg_probs = [i[0][1] for i in all_probs]\n",
    " \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# pos\n",
    "plt.hist(pos_probs, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.xlabel('predicted_values')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Probabilities for positive cases')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    " \n",
    "# neg\n",
    "plt.hist(neg_probs, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.xlabel('predicted_values')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Probabilities for negative cases')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/neg.png)\n",
    "![caption](files/pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down - sampling: reduce samples of negative cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    " \n",
    "RATIO_ADJUST = 2.0 ## ratio of pos to neg in the df_subsample\n",
    " \n",
    "counts = trainingData.select('binary_response').groupBy('binary_response').count().collect()\n",
    "higherBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * higherBound)\n",
    " \n",
    "randGen = lambda x: randint(0, higherBound) if x == 'Positive' else -1\n",
    " \n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    "trainingData = trainingData.withColumn(\"randIndex\", udfRandGen(\"binary_response\"))\n",
    "df_subsample = trainingData.filter(trainingData['randIndex'] < TRESHOLD_TO_FILTER)\n",
    "df_subsample = df_subsample.drop('randIndex')\n",
    " \n",
    "print(\"Distribution of Pos and Neg cases of the down-sampled training data are: \\n\", df_subsample.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Pos and Neg cases of the down-sampled training data are: \n",
    " [Row(label=1.0, count=144014), Row(label=0.0, count=287482)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test down-sampling result - slightly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training and prediction\n",
    "rf = RF(labelCol='label', featuresCol='features',numTrees=200)\n",
    "fit = rf.fit(df_subsample)\n",
    "transformed = fit.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## results and evaluation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = transformed.select(['probability', 'label'])\n",
    " \n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC score is (@numTrees=200):  0.6463328674547113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of down-samplings: \n",
    "### 1) down-sampling N times. \n",
    "### 2) each time, train our model and make prediction on test data\n",
    "### 3) take the average of all N predictions on test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    " \n",
    "RATIO_ADJUST = 3.0 ## ratio of pos to neg in the df_subsample\n",
    "TOTAL_MODELS = 10\n",
    "total_results = None\n",
    "final_result = None\n",
    " \n",
    "#counts = trainingData.select('binary_response').groupBy('binary_response').count().collect()\n",
    "highestBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * highestBound)\n",
    "## UDF\n",
    "randGen = lambda x: randint(0, highestBound) if x == 'Positive' else -1\n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    " \n",
    "## ensembling\n",
    "for N in range(TOTAL_MODELS):\n",
    "    print(\"Round: \", N)\n",
    "    trainingDataIndexed = trainingData.withColumn(\"randIndex\", udfRandGen(\"binary_response\"))\n",
    "    df_subsample = trainingDataIndexed.filter(trainingDataIndexed['randIndex'] < TRESHOLD_TO_FILTER).drop('randIndex')\n",
    "    ## training and prediction\n",
    "    rf = RF(labelCol='label', featuresCol='features',numTrees=200)\n",
    "    fit = rf.fit(df_subsample)\n",
    "    transformed = fit.transform(testData)\n",
    "    result_pair = transformed.select(['probability', 'label'])\n",
    "    result_pair = result_pair.collect()\n",
    "    this_result = np.array([float(i[0][1]) for i in result_pair])\n",
    "    this_result = list(this_result.argsort().argsort() / (float(len(this_result) + 1)))\n",
    " \n",
    "    ## sum up all the predictions, and average to get final_result\n",
    "    if total_results is None:\n",
    "       total_results = this_result\n",
    "    else:\n",
    "       total_results = [i+j for i, j in zip(this_result, total_results)]\n",
    "    final_result = [i/(N+1) for i in total_results]\n",
    " \n",
    "    results_list = [(float(i), float(j[1])) for i, j in zip(final_result, result_pair)]\n",
    "    scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "    metrics = metric(scoreAndLabels)\n",
    " print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round:  0\n",
    "The ROC score is (@numTrees=200):  0.6456296366007628\n",
    "Round:  1\n",
    "The ROC score is (@numTrees=200):  0.6475210701955153\n",
    "Round:  2\n",
    "The ROC score is (@numTrees=200):  0.6488169677072237\n",
    "Round:  3\n",
    "The ROC score is (@numTrees=200):  0.6490333812262444\n",
    "Round:  4\n",
    "The ROC score is (@numTrees=200):  0.6490997896881725\n",
    "Round:  5\n",
    "The ROC score is (@numTrees=200):  0.648347665785477\n",
    "Round:  6\n",
    "The ROC score is (@numTrees=200):  0.6486544723987375\n",
    "Round:  7\n",
    "The ROC score is (@numTrees=200):  0.6492410064530146\n",
    "Round:  8\n",
    "The ROC score is (@numTrees=200):  0.6493154941849306\n",
    "Round:  9\n",
    "The ROC score is (@numTrees=200):  0.6483560027574977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
